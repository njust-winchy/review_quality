# A Unified Framework for Multi-Dimensional Evaluation of Peer Review Quality in Academic Paper


## Overview

**Dataset and source code for paper "A Unified Framework for Multi-Dimensional Evaluation of Peer Review Quality in Academic Paper".**
The overall framework of this study is shown in here.<br>
![image](https://github.com/user-attachments/assets/0f254af7-3006-4ecb-9800-066451a121da)

## Dataset
The raw data and calculated data can be obtained from here().<br>
## Directory structure

<pre>
review_quality                                    Root directory
├── claim_tagging                                 Code for claim tagging model
│   ├── data_preprocess.py                        Process the dataset into training and testing sets
│   ├── dataset.py                                Tokenize the data
│   ├── model.py                                  Model architecture
│   ├── params.py                                 Set training parameters
│   ├── test.py                                   Run on the test set
│   ├── train.py                                  Main function for model training
│   ├── utils.py                                  Training process
├── confidence                                    Code for confidence model
│   ├── data_process.py                           Process the dataset into training and testing sets
│   ├── load_data.py                              Tokenize the data
│   ├── main.py                                   Main function for model training
│   ├── model.py                                  Model architecture
│   ├── predict.py                                Run on the test set
│   ├── read_hedge.py                             Process the raw dataset into the required format
│   ├── utils.py                                  Text processing methods
├── constructive                                  Code for constructiveness model
│   ├── preprocess_data.py                        Process the dataset into training and testing sets
│   ├── dataset.py                                Tokenize the data
│   ├── model.py                                  Model architecture
│   ├── params.py                                 Set training parameters
│   ├── test.py                                   Run on the test set
│   ├── train.py                                  Main function for model training
│   ├── utils.py                                  Training process
├── evidence_identification                       Code for evidence identification model
│   ├── data_preprocess.py                        Process the dataset into training and testing sets
│   ├── dataset.py                                Tokenize the data
│   ├── model.py                                  Model architecture
│   ├── params.py                                 Set training parameters
│   ├── test.py                                   Run on the test set
│   ├── train.py                                  Main function for model training
│   ├── utils.py                                  Training process
├── politeness                                    Kindness model
│   └── politeness_predition.ipynb                Politeness score prediction pipeline
├── review_evaluate                               Quality score calculation
│   └── model                                     Pre trained model architecture
│   │   ├── claim_model.py                        Claim tagging model architecture
│   │   ├── constructive_model.py                 Constructiveness tagging model architecture
│   │   ├── evi_model.py                          Evidence identification model architecture
│   │   ├── hedge_model.py                        Confidence model architecture
│   ├── case_prompt.txt                           Prompt for case study
│   ├── case_select.py                            Select samples for case study
│   ├── case_study.py                             Code for case study
│   ├── feture_extract.py                         Text feature extraction
│   ├── llm_chunking.py                           Code for review chunking
│   ├── politeness_calculate.py                   Kindness score calculation
│   ├── politeness_preprocess.py                  Processing data for Kindness model
│   ├── Preprocess_RAG.py                         Retrieve examples for review chunking
│   ├── Prompt4llmchunking.txt                    Prompt for review chunking
│   ├── quality_analysis.py                       Quality score analysis
│   ├── quality_score.py                          Code for quality score calculation
│   ├── review2chunk.py                           Process the results generated by LLM
│   ├── score_analysis.py                         Five dimensional score analysis
│   ├── score_calculate.py                        Five dimensional score calculation
│
└── README.md
</pre>


## Dependency packages
System environment is set up according to the following configuration:
- transformers==4.16.2
- nltk==3.6.7
- matplotlib==3.5.1
- scikit-learn==1.1.3
- pytorch 2.0.1
- tqdm 4.65.0
- numpy 1.24.1

## Acknowledgement

We express our gratitude to the team at openreview.net for their dedication to advancing transparency and openness in scientific communication. We utilized the aspect identifying tool developed by Yuan et al.（2022）(https://github.com/neulab/ReviewAdvisor).
The datasets we use come from Guo et al.（2023）(https://github.com/YanzhuGuo/SubstanReview.), Dycke et al.（2023）(https://github.com/UKPLab/nlpeer), Bharti et al.（2023）(https://github.com/PrabhatkrBharti/PolitePEER.git), Ghosal et al.（2022）(https://github.com/Tirthankar-Ghosal/HedgePeer-Dataset) Bharti et al. (2022) (https://github.com/PrabhatkrBharti/BetterPR.git)

>Yuan, W., Liu, P., & Neubig, G. (2022). Can we automate scientific reviewing?. Journal of Artificial Intelligence Research, 75, 171-212.<br>

>Yanzhu Guo, Guokan Shang, Virgile Rennard, Michalis Vazirgiannis, and Chloé Clavel. 2023. Automatic Analysis of Substantiation in Scientific Peer Reviews. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 10198–10216, Singapore. Association for Computational Linguistics.<br>

>Nils Dycke, Ilia Kuznetsov, and Iryna Gurevych. 2023. NLPeer: A Unified Resource for the Computational Study of Peer Review. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5049–5073, Toronto, Canada. Association for Computational Linguistics.<br>

>Bharti, P.K., Navlakha, M., Agarwal, M. et al. PolitePEER: does peer review hurt? A dataset to gauge politeness intensity in the peer reviews. Lang Resources & Evaluation 58, 1291–1313 (2024). https://doi.org/10.1007/s10579-023-09662-3<br>

>T. Ghosal, K. K. Varanasi and V. Kordoni, "HedgePeer: A Dataset for Uncertainty Detection in Peer Reviews," 2022 ACM/IEEE Joint Conference on Digital Libraries (JCDL), Cologne, Germany, 2022, pp. 1-5.<br>

>Bharti, P.K., Ghosal, T., Agarwal, M., Ekbal, A. (2022). BetterPR: A Dataset for Estimating the Constructiveness of Peer Review Comments. In: Silvello, G., et al. Linking Theory and Practice of Digital Libraries. TPDL 2022. Lecture Notes in Computer Science, vol 13541. Springer, Cham. https://doi.org/10.1007/978-3-031-16802-4_53<br>

## Citation
Please cite the following paper if you use this code and dataset in your work.




