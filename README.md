# review_quality


## Overview

**Dataset and source code for paper "".**



## Model overview

This study proposes a framework consisting of knowledge-guided fusion module and two paralleled sub-tasks. The two paralleled sub-tasks are originality prediction (OP) and decision prediction (DP) respectively.<br>


## Directory structure

<pre>
review_quality                                    Root directory
├── claim_tagging                                 Code for claim tagging model
│   ├── data_preprocess.py                        Process the dataset into training and testing sets
│   ├── dataset.py                                Tokenize the data
│   ├── model.py                                  Model architecture
│   ├── params.py                                 Set training parameters
│   ├── test.py                                   Run on the test set
│   ├── train.py                                  Main function for model training
│   ├── utils.py                                  Training process
├── confidence                                    Code for confidence model
│   ├── data_process.py                           Process the dataset into training and testing sets
│   ├── load_data.py                              Tokenize the data
│   ├── main.py                                   Main function for model training
│   ├── model.py                                  Model architecture
│   ├── predict.py                                Run on the test set
│   ├── read_hedge.py                             Process the raw dataset into the required format
│   ├── utils.py                                  Text processing methods
├── constructive                                  Code for constructiveness model
│   ├── preprocess_data.py                        Process the dataset into training and testing sets
│   ├── dataset.py                                Tokenize the data
│   ├── model.py                                  Model architecture
│   ├── params.py                                 Set training parameters
│   ├── test.py                                   Run on the test set
│   ├── train.py                                  Main function for model training
│   ├── utils.py                                  Training process
├── evidence_identification                       Code for evidence identification model
│   ├── data_preprocess.py                        Process the dataset into training and testing sets
│   ├── dataset.py                                Tokenize the data
│   ├── model.py                                  Model architecture
│   ├── params.py                                 Set training parameters
│   ├── test.py                                   Run on the test set
│   ├── train.py                                  Main function for model training
│   ├── utils.py                                  Training process
├── politeness                                    Kindness model
│   └── politeness_predition.ipynb                Politeness score prediction pipeline
├── review_evaluate                               Quality score calculation
│   └── model                                     Pre trained model architecture
│   │   ├── claim_model.py                        Claim tagging model architecture
│   │   ├── constructive_model.py                 Constructiveness tagging model architecture
│   │   ├── evi_model.py                          Evidence identification model architecture
│   │   ├── hedge_model.py                        Confidence model architecture
│   ├── case_prompt.txt                           Prompt for case study
│   ├── case_select.py                            Select samples for case study
│   ├── case_study.py                             Code for case study
│   ├── feture_extract.py                         Text feature extraction
│   ├── llm_chunking.py                           Code for review chunking
│   ├── politeness_calculate.py                   Kindness score calculation
│   ├── politeness_preprocess.py                  Processing data for Kindness model
│   ├── Preprocess_RAG.py                         Retrieve examples for review chunking
│   ├── Prompt4llmchunking.txt                    Prompt for review chunking
│   ├── quality_analysis.py                       Quality score analysis
│   ├── quality_score.py                          Code for quality score calculation
│   ├── review2chunk.py                           Process the results generated by LLM
│   ├── score_analysis.py                         Five dimensional score analysis
│   ├── score_calculate.py                        Five dimensional score calculation
│
└── README.md
</pre>


## Dependency packages
System environment is set up according to the following configuration:
- transformers==4.16.2
- nltk==3.6.7
- matplotlib==3.5.1
- scikit-learn==1.1.3
- pytorch 2.0.1
- tqdm 4.65.0
- numpy 1.24.1

## Acknowledgement

We express our gratitude to the team at openreview.net for their dedication to advancing transparency and openness in scientific communication. We utilized the aspect identifying tool developed by Yuan et al.（2022）(https://github.com/neulab/ReviewAdvisor).

>Yuan, W., Liu, P., & Neubig, G. (2022). Can we automate scientific reviewing?. Journal of Artificial Intelligence Research, 75, 171-212.<br>


## Citation
Please cite the following paper if you use this code and dataset in your work.




